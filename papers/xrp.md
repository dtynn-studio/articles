# XRP 阅读
[《XRP: In-Kernel Storage Functions with eBPF》](https://www.usenix.org/system/files/osdi22-zhong_1.pdf) 是一篇来自 `OSDI'22` 的论文，介绍了一种通过 Linux eBPF 来充分发挥高速存储设备性能的思路。

太长不读，请跳到 [思考与解读](#思考与解读)

## 内容梗概


### 背景与动机
作者认为，在使用目前的高速存储设备（能够提供个位数微秒级别的延迟和上百万IOPS）时，内核软件栈已经成为了性能瓶颈。作为说明，作者列出了在使用`Intel 傲腾 P5800X` 进行 512字节随机读的场景下，各个环节的平均延迟：

|环节|平均延迟|占比|
|----|----|----|
| kernel crossing | 351ns | 5.6% |
| read syscall | 199ns | 3.2% |
| ext4 | 2006ns | 32.0% |
| bio | 379ns | 6.0% |
| NVMe driver | 113ns | 1.8% |
| storage device | 3224ns | 51.4% |
| total | 6.27us | 100% |

可以看出，前五个环节（作者认为的、软件层面的开销）总共达到了 48.6% 的占比。

同时，作者认为，作为此场景下的一种常见思路，内核旁路方案也有一些不足，例如：
1. 需要构建用户态文件系统来提供访问隔离等安全机制
2. 因为无法直接接受 I/O 完成时的中断信号，因此需要通过队列 polling 机制
3. 由于 2) 的存在，又导致：
   - 每个应用进程需要维护自己的 polling 线程，无法共享，可能导致 CPU 的浪费
   - 当一个 CPU 核上存在多个 polling 线程时，竞争的发生和同步的缺失会共同导致尾部延迟与全局吞吐的显著降低


对于 BPF 这样一种可以有效避免数据在内核和用户空间之间来回传递的机制，作者认为它可以用于“获取一系列辅助性数据并仅返回最终结果”的场景，例如遍历B树索引获取数据。
通过 BPF 程序可以将节点检索的过程维持在内核中，而不必频繁在内核态与用户态之间交互。

作者将这种优化命名为 XRP（eXpress Resubmission Path）。
具体来说，XRP 就是将一些对间接数据的使用逻辑下沉到靠近存储的位置，从而降低在内核态和用户态之间交互的开销。

一些额外的说明：
- 理论上来说，XRP 可以位于 IO 路径的任何一层，但事实上越靠近硬件层，效果越好（作者最终将它置于 NVMe 驱动层）
- `io_uring` 作为异步 IO 框架，它的每一个 IO 请求同样可以受益于 XRP

### 设计思路
由于需要尽可能靠近硬件层，导致 XRP 存在缺少文件系统上下文的问题，因此面临一些挑战：

#### 挑战 1：地址转译与安全
在驱动层， XRP 需要按照硬件块来访问数据，但在应用层，通常使用文件和偏移量来访问，两者需要通过文件系统进行转译。

此外，驱动层能够访问任何硬件块数据，需要借助文件系统来完成权限和隔离的约束。

#### 挑战 2：并发与缓存
让 XRP 支持来自文件系统的并发读写是一个巨大的挑战，具体体现在：
- 文件系统的写仅仅影响页缓存（page cache），这是 XRP 无法访问的
- 对数据的写可能导致读请求失效

并发问题可以通过锁来解决，但是在驱动层访问锁的开销无疑是巨大的。

#### 观察：大多数保存在磁盘上的数据结构是稳定的
根据观察，很多存储引擎所使用的数据结构是相对稳定的，要么完全不会进行原址更新（如LSM的索引文件），要么不会频繁更新（如B树索引在某些常见的读写负载场景）。
此外，这些存储引擎都会将索引保存在少量大文件中，且每条索引都不会跨文件保存。

这些实际情况使得以上两类挑战可以在一些特定场景下被解决。


#### 设计原则
基于上面的挑战与现实场景，作者提出了 XRP 的设计原则：
- 一次访问一个文件
- 面向稳定的数据结构
- 使用用户层的缓存机制
- 异常情况下回退

### XRP 的设计与实现
作者基于 Linux eBPF 和 ext4 文件系统介绍了 XRP 的一些实现细节和局限。

#### 1. IO 请求重新提交的逻辑
在NVMe 请求完成的中断处理函数中，XRP 会将数据解析为应用层使用的数据结构，并将指向下一个数据块的指针转译为硬件块的定位信息，直接向设备发起下一次读取请求。
这个过程会持续循环直到获取到最终结果，或任何异常发生。

在这个逻辑中，我们需要考虑以下环节：
1. BPF 钩子函数
2. BPF 校验函数：用于校验数据语义的有效性
3. 元数据摘要：供文件系统提供和更新逻辑定位信息和硬件块定位信息之间的映射
4. 读取请求的重发：通过复用请求数据结构来避免内存分配

#### 2. 同步的局限
受限于 BPF 所能使用的同步机制（目前仅能使用一种受限的自旋锁），多个读写请求之间的复杂同步是无法完成的。

用户可以使用 BPF 的原子操作来自行实现自旋锁，继续使用应用和 BPF 程序之间的同步控制。

另一种选择是使用 RCU（read-copy-update）。

#### 3. 与 Linux 调度器交互
- 进程调度：多个共享了CPU核的进程会被 Linux 的 CFS 所干扰。看上去，这种情况是由于高速存储设备更为频繁地产生了中断信号的导致的。这种情况不仅仅出现在 eBPF 中。

- IO 调度：XRP 绕过了系统层面的 IO 调度器，当它仍然会受到存储设备自己的调度器影响

### 实现案例
作者完成了两种具体实现来进行验证：
- 一种名为 `BPF-KV` 的基于B树索引的KV数据库
- 基于 mongo 所使用的存储引擎 `WiredTiger` 加入 XRP 的改造结果

### 验证结果
作者基于上面的实现对这几个问题进行了验证：
1. 为存储场景使用 BPF 的开销情况
2. XPR 扩展到多线程的结果
3. XRP 可以支持的操作种类
4. XRP 是否能加速真实场景的 KV 存储

验证过程中，使用四类实现相互对比：
1. 基于 XRP 的实现
2. 基于 SPDK 的实现
3. 基于标准的 `read` 系统调用
4. 基于标准的 `io_uring` 系统调用

#### 关于延迟
在B+树检索的场景中，统计随树高度增加，检索的延迟数值和变化情况。
四类实现基本都表现出线性的特征，延迟大致为 `t0 + delta * (depth-1)`。

根据统计
1. `read` 和 `io_uring` 的延迟表现基本相同，t0 在 13.5us 左右， delta 7~8us
2. SPDK 实现表现最好， t0 为 5.2us，delta 在 3us 左右
3. XRP 实现略低于 SPDK 实现， t0 为 10.7us，delta 在 3us 左右

根据分析：
- XRP 的初始延迟高于 SPDK，是由于 XRP 的第一次请求要多出一次系统调用，存在额外开销
- 之后随高度——即要访问的数据块数量——增加的延迟，接近于硬件设备的延迟，与 SPDK 接近

值得注意的是，在线程数量超过 CPU 核数的情况下，SPDK 方案，相比其他方案， 99百分位数延迟和99.9百分位数延迟会出现较大浮动，表现为延迟超过1ms 的请求数量占比大幅增加：
- 7 线程时，0.03% vs <0.01%
- 24 线程时，0.28% vs <0.01%

#### 关于吞吐
单线程情况下，吞吐的性能表现与延迟类似：
- `read` 和 `io_uring` 表现接近，表现较差
- SPDK 表现最好，但是随树高度增加，变现下降较快
- XRP 介于上述两种情况之间

在多线程情况下，以 `线程数 = CPU 核数` 为分界：
- 在线程数达到分界值之前，各实现基本符合随线程数量线性增长的特征
- 在线程数超过分界值之后：
  - `read` 和 `io_uring` 实现仍然有所增长，但增长幅度放缓
  - SPDK 实现随线程数量继续增加，性能反而下降
  - XRP 基本保持继续线性增长

#### 线程扩展
在这项验证中，使用 集成了 XRP 的 io_uring 与 SPDK 进行对比，结论为：
- 在线程数不超过 CPU 核数的情况下，两种方案都能达到接近硬件极限的吞吐量
- 在线程数超过 CPU 核数后，`XRP + io_uring` 方案吞吐量几乎没有发生变化，而 SPDK 方案随线程数量增加，吞吐量表现线性下降

出现这种情况，是由于 SPDK 所用到的 polling 线程倾向于更长地占据 CPU，相互之间形成竞争

#### 范围请求
在这项验证中，使用 XRP 实现和 `read` 实现，对比两者的平均延迟和吞吐量。

- XRP 方案的延迟和吞吐表现均优于 `read` 方案。
- XRP 的表现与所使用的数据结构有关，具体表现为，数据体内包含的指针数量，即一次读取后可以立即发起的、再次读取的数据块数量

#### 基于 WiredTiger 的改造验证
吞吐量的表现，根据数据集的不同，XRP 能提供的优化效果有所不同，结论如下：
- 对于常规数据集，XRP 仍然能提供优化，但是效果不如在 `BPF-KV` 中的表现，这是由于 WiredTiger 自身所包含的缓存机制，使得测试过程中，IO 的比例降低
- 对于一些具备特殊特征的数据集，XRP 几乎无法提供优化：
  - 仅读取最新插入的数据：受到缓存机制的影响，XRP 无法发挥作用
  - 大量扫描数据：受限于 WiredTiger 的扫描实现，XRP 几乎每次都仅能处理单个数据块

尾部延迟的表现中，XRP 实现能提供较大比例的优化，但是和在吞吐量的验证中一样，受到缓存命中比例的大幅影响。

### 其他工作
作者罗列了一些可能的衍生方向：
- 使用BPF加速IO（以网络IO为典型场景）
- 内核旁路系统
- 就近计算
- 可扩展的操作系统和文件系统

## 思考与解读
- XRP 类似的方案在一些特定场景下确实表现出相对传统实现的优化，但它的优缺点都非常明显：
  - 优点是在存在大量IO线程（较多不同的IO密集型应用）的情况下，表现相对稳定
  - 缺点是使用限制较多，与应用的具体实现耦合较大，且对应用之外的环节（如文件系统）有侵入性

- 在绝大多数情况下，`SPDK` 方案都表现出非常强劲的性能，且对开发更友好，具体表现在：
  - 基于传统的文件系统抽象，对开发者隐去了具体细节
  - 几乎只在大量IO密集型应用发生争抢的时候性能有明显下降，而这种情况很少出现在具体业务部署中

综上，如果希望 XRP 方案能够被应用到更广的范围，可以考虑的一种思路是：将文件系统与可定制的 XRP 框架集成，由应用注入数据块的处理方式，同时隐去文件系统和XRP之间的元数据交互细节。

当然，基于 BPF 肯定会有更多针对本地存储 IO 的优化思路涌现出来。
